---
layout: post
read_time: true
show_date: true
title: "What is U-net? "
date:   2021-07-20 21:32:20 -0600
description: Recording an algorithm of image processing
img: posts/20210718/cover.png
tags: [U-net, image processing, machine learning]
author: Lydia Shaw
github:  LydiaCShaw
mathjax: yes
---
## What is U-net?
The u-net is convolutional network architecture for fast and precise segmentation of images especially useful in biomedical images segmentation. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories by a large margin.
### U-Net
![imageunet](.\assets\img\posts\20210718\cover.png)
*U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.*

The structure of U-Net is shown in the figure. The left side can be regarded as an encoder, and the right side can be regarded as a decoder.

The encoder has four sub-modules, each of which contains two convolutional layers. After each sub-module, there is a down-sampling layer implemented by max pool. 

The resolution of the input image is 572x572, and the resolutions of modules 1-5 are 572x572, 284x284, 140x140, 68x68 and 32x32 respectively. Since the convolution uses the valid mode, the resolution of the next sub-module here is equal to (resolution of the previous sub-module -4)/2. 

The decoder contains four sub-modules, and the resolution is sequentially increased by upsampling until it is consistent with the resolution of the input image (because the convolution uses the valid mode, the actual output is smaller than the input image).

The network also uses a skip connection to connect the up-sampling result with the output of the sub-module with the same resolution in the encoder as the input of the next sub-module in the decoder.
Karas U-Net codes:[github link](https://github.com/zhixuhao/unet)

### 3D U-Net
3D U-Net is a simple extension of U-Net, applied to 3D image segmentation. Compared with U-Net, this network only uses three downsampling operations, and batch normalization is used after each convolutional layer, but both 3D U-Net and U-Net do not use dropout.
![imageunet](.\assets\img\posts\20210718\3dunet.jpg)
Pytorch 3D U-Net codes:[github link](https://github.com/wolny/pytorch-3dunet)

### TernausNet
TernausNet is called "TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation" .
![imageunet](.\assets\img\posts\20210718\tern.png)
The network replaces the encoder in U-Net with VGG11 and pre-trains on ImageNet.
TernausNet codes:[github link](https://github.com/ternaus/TernausNet)
### Res-UNet and Dense U-Net
Res-UNet and Dense-UNet are inspired by residual connection and dense connection respectively, and replace each sub-module of U-Net with the form of residual connection and dense connection respectively.  

Res-UNet is used for retinal image segmentation. Its structure is shown in the figure below, where the solid gray lines represent the residual connections added in each module.
![imageunet](.\assets\img\posts\20210718\res.jpg)

Dense connection means that the output of a certain layer in the sub-module is used as a part of the input of several subsequent layers, and the input of a certain layer comes from the combination of the output of several previous layers. The figure below is an example of dense connection. In this article, each sub-module of U-Net is replaced with such a densely connected module, and Fully Dense U-Net is proposed to remove artifacts in the image.
![imageunet](.\assets\img\posts\20210718\dense.jpg)
###  MultiResUNet
MultiResUNet proposed a MutiRes module combined with U-Net. The MutiRes module is an extension of the residual connection. In this module, three 3x3 convolution results are spliced together as a combined feature map, and then added to the result obtained by 1x1 convolution of the input feature map.
![imageunet](.\assets\img\posts\20210718\mult.jpg)
In addition to the MultiRes module, this network also proposes a residual path (ResPath), so that the features of the encoder are concatenated with the corresponding features in the decoder before some additional convolution operations are performed, as shown in the following figure. The author believes that the feature in the encoder is a low-level feature due to the shallower convolutional layer, while the corresponding feature in the decoder is a higher-level feature due to the deeper convolutional layer. There is a big difference in semantics between the two. , It is speculated that it is not appropriate to splice the two directly. Therefore, additional ResPath is used to make the two have the same depth before splicing. In ResPath1, 2, 3, 4, 4, 3, 2, and 1 convolutional layers are used respectively.
![imageunet](.\assets\img\posts\20210718\multi.jpg)
![imageunet](.\assets\img\posts\20210718\multi2.png)
MultiResUNet[github link](https://github.com/nibtehaz/MultiResUNet)
### R2U-Net
The full name of R2U-Net is called Recurrent Residual CNN-based U-Net. This method combines residual connection and cyclic convolution to replace the original sub-module in U-Net.
![imageunet](.\assets\img\posts\20210718\r2u.jpg)
The circular arrow indicates the circular connection. The following figure shows the internal structure of several different sub-modules, (a) is the method used in the conventional U-Net, (b) is the convolutional layer containing the activation function on the basis of (a), ( c) is the method of using residual connection, (d) is the cyclic residual convolution module that combines (b) and (c) proposed in this article.
![imageunet](.\assets\img\posts\20210718\r2u2.jpg)
R2U-Net codes:[github link](https://github.com/LeeJunHyun/Image_Segmentation#r2u-net)

### Attention U-Net
Attention U-Net introduced the attention mechanism in U-Net. Before splicing the features at each resolution of the encoder with the corresponding features in the decoder, an attention module was used to readjust the output features of the encoder. This module generates a gating signal to control the importance of features at different spatial positions, as shown by the red circle in the figure below.
![imageunet](.\assets\img\posts\20210718\attu.png)
The inside of the attention module of this method is shown in the following figure. The module combines ReLU and Sigmoid through 1x1x1 convolution to generate a weight map α, which is corrected by multiplying the features in the encoder.
![imageunet](.\assets\img\posts\20210718\atture.jpg)
Attention U-Net codes:[github link](https://github.com/ozan-oktay/Attention-Gated-Networks)



## Reference
[1][U-Net: Convolutional Networks for Biomedical Image Segmentation](https://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a/)

[2][@taigw zhihu](https://zhuanlan.zhihu.com/p/57530767)

[3]Çiçek, Özgün, Ahmed Abdulkadir, Soeren S. Lienkamp, Thomas Brox, and Olaf Ronneberger. "3D U-Net: learning dense volumetric segmentation from sparse annotation." In International conference on medical image computing and computer-assisted intervention, pp. 424-432. Springer, Cham, 2016.
[4] Isensee, Fabian, Philipp Kickingereder, Wolfgang Wick, Martin Bendszus, and Klaus H. Maier-Hein. "No new-net." In International MICCAI Brainlesion Workshop, pp. 234-244. Springer, Cham, 2018.

[5] Xiao, Xiao, Shen Lian, Zhiming Luo, and Shaozi Li. "Weighted Res-UNet for High-Quality Retina Vessel Segmentation." In 2018 9th International Conference on Information Technology in Medicine and Education (ITME), pp. 327-331. IEEE, 2018.

[6] Guan, Steven, Amir Khan, Siddhartha Sikdar, and Parag V. Chitnis. "Fully Dense UNet for 2D Sparse Photoacoustic Tomography Artifact Removal." arXiv preprint arXiv:1808.10848 (2018).

[7] Ibtehaz, Nabil, and M. Sohel Rahman. "MultiResUNet: Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation." arXiv preprint arXiv:1902.04049 (2019).

[8] Alom, Md Zahangir, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha, and Vijayan K. Asari. "Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation." arXiv preprint arXiv:1802.06955 (2018).

[9] Oktay, Ozan, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori et al. "Attention U-Net: learning where to look for the pancreas." arXiv preprint arXiv:1804.03999 (2018).

